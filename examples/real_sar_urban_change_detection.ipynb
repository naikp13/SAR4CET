{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real SAR Urban Change Detection with Sentinel-1 Data\n",
    "\n",
    "This notebook demonstrates urban change detection using real Sentinel-1 SAR data.\n",
    "We will download, preprocess, and analyze time-series SAR patches to detect changes.\n",
    "\n",
    "## Requirements:\n",
    "- Copernicus Open Access Hub account (https://scihub.copernicus.eu/dhus/)\n",
    "- Set environment variables: COPERNICUS_USER and COPERNICUS_PASSWORD\n",
    "- Internet connection for data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "# Install SAR4CET and dependencies\n",
    "try:\n",
    "    import sar4cet\n",
    "except ImportError:\n",
    "    print('Installing SAR4CET...')\n",
    "    install_package('sentinelsat')\n",
    "    install_package('rasterio')\n",
    "    install_package('geopandas')\n",
    "    install_package('matplotlib')\n",
    "    install_package('numpy')\n",
    "    install_package('scipy')\n",
    "    install_package('scikit-image')\n",
    "    install_package('opencv-python')\n",
    "    \n",
    "    # Install SAR4CET package\n",
    "    import os\n",
    "    \n",
    "    # Method 1: Try installing from GitHub\n",
    "    try:\n",
    "        print('Installing SAR4CET from GitHub...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'git+https://github.com/naik15/SAR4CET.git'])\n",
    "        print('Successfully installed from GitHub!')\n",
    "    except subprocess.CalledProcessError:\n",
    "        print('GitHub install failed, trying alternative methods...')\n",
    "        \n",
    "        # Method 2: Try local installation if we're in a local environment\n",
    "        try:\n",
    "            if os.path.exists('../setup.py'):\n",
    "                print('Found local setup.py, installing locally...')\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-e', '..'])\n",
    "                print('Successfully installed locally!')\n",
    "            else:\n",
    "                raise FileNotFoundError('No local setup.py found')\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            # Method 3: Clone and install\n",
    "            print('Cloning repository and installing...')\n",
    "            if not os.path.exists('SAR4CET'):\n",
    "                subprocess.check_call(['git', 'clone', 'https://github.com/naik15/SAR4CET.git'])\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-e', './SAR4CET'])\n",
    "            print('Successfully installed from cloned repository!')\n",
    "\n",
    "print('All packages installed successfully!')\n",
    "\n",
    "# Verify SAR4CET installation\n",
    "try:\n",
    "    import sar4cet\n",
    "    print(f'SAR4CET version: {getattr(sar4cet, \"__version__\", \"unknown\")}')\n",
    "    print('SAR4CET successfully imported!')\n",
    "except ImportError as e:\n",
    "    print(f'Warning: SAR4CET import failed: {e}')\n",
    "    print('Please restart the runtime and try again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import SAR4CET modules\n",
    "from sar4cet import preprocessing, change_detection, visualization, utils\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Copernicus Hub Credentials\n",
    "\n",
    "You need to register at https://scihub.copernicus.eu/dhus/ and set your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Copernicus credentials\n",
    "# Option 1: Set environment variables (recommended)\n",
    "# export COPERNICUS_USER=\"your_username\"\n",
    "# export COPERNICUS_PASSWORD=\"your_password\"\n",
    "\n",
    "# Option 2: Set them here (not recommended for security)\n",
    "# os.environ['COPERNICUS_USER'] = 'your_username'\n",
    "# os.environ['COPERNICUS_PASSWORD'] = 'your_password'\n",
    "\n",
    "# Check if credentials are set\n",
    "if 'COPERNICUS_USER' in os.environ and 'COPERNICUS_PASSWORD' in os.environ:\n",
    "    print('Credentials found!')\n",
    "    print(f\"Username: {os.environ['COPERNICUS_USER']}\")\n",
    "else:\n",
    "    print('WARNING: Copernicus credentials not found!')\n",
    "    print('Please set COPERNICUS_USER and COPERNICUS_PASSWORD environment variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Area of Interest and Time Period\n",
    "\n",
    "We'll focus on a small urban area to keep download sizes manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define area of interest (small patch for demonstration)\n",
    "# Example: San Francisco Bay Area (small subset)\n",
    "aoi_bbox = [-122.45, 37.75, -122.40, 37.80]  # [lon_min, lat_min, lon_max, lat_max]\n",
    "\n",
    "# Alternative areas (uncomment one):\n",
    "# London, UK (small patch)\n",
    "# aoi_bbox = [-0.15, 51.50, -0.10, 51.52]\n",
    "\n",
    "# Berlin, Germany (small patch)\n",
    "# aoi_bbox = [13.35, 52.50, 13.40, 52.52]\n",
    "\n",
    "# Time period (keep short to limit data volume)\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-06-30'\n",
    "\n",
    "print(f\"Area of Interest: {aoi_bbox}\")\n",
    "print(f\"Time Period: {start_date} to {end_date}\")\n",
    "\n",
    "# Create AOI geometry for visualization\n",
    "aoi_geom = box(aoi_bbox[0], aoi_bbox[1], aoi_bbox[2], aoi_bbox[3])\n",
    "aoi_gdf = gpd.GeoDataFrame([1], geometry=[aoi_geom], crs='EPSG:4326')\n",
    "\n",
    "# Plot AOI\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "aoi_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=2)\n",
    "ax.set_title('Area of Interest')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search and Download Sentinel-1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create download directory\n",
    "download_dir = 'sentinel1_data'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Search for Sentinel-1 data\n",
    "    print('Searching for Sentinel-1 data...')\n",
    "    search_results = preprocessing.search_sentinel1(\n",
    "        aoi=aoi_bbox,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        polarization='VV VH',\n",
    "        product_type='GRD'\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(search_results)} Sentinel-1 products\")\n",
    "    \n",
    "    # Display search results\n",
    "    if len(search_results) > 0:\n",
    "        print('\\nFirst 5 products:')\n",
    "        for i, (product_id, product_info) in enumerate(list(search_results.items())[:5]):\n",
    "            print(f\"{i+1}. {product_info['title']} - {product_info['beginposition']}\")\n",
    "    \n",
    "    # Limit downloads to first 3-5 products to keep data manageable\n",
    "    max_downloads = min(5, len(search_results))\n",
    "    limited_results = dict(list(search_results.items())[:max_downloads])\n",
    "    \n",
    "    print(f\"\\nDownloading {max_downloads} products...\")\n",
    "    downloaded_files = preprocessing.download_sentinel1(\n",
    "        aoi=aoi_bbox,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        download_dir=download_dir,\n",
    "        max_products=max_downloads\n",
    "    )\n",
    "    \n",
    "    print(f\"Downloaded {len(downloaded_files)} files\")\n",
    "    for file in downloaded_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during download: {e}\")\n",
    "    print(\"\\nFalling back to simulated data for demonstration...\")\n",
    "    \n",
    "    # Create simulated data as fallback\n",
    "    downloaded_files = []\n",
    "    for i in range(5):\n",
    "        # Create simulated SAR data\n",
    "        np.random.seed(42 + i)\n",
    "        base_image = np.random.gamma(2, 0.5, (512, 512))\n",
    "        \n",
    "        # Add some urban-like structures\n",
    "        base_image[100:150, 100:150] *= 1.5  # Building block\n",
    "        base_image[200:250, 200:300] *= 1.3  # Another area\n",
    "        \n",
    "        # Add temporal changes\n",
    "        if i > 2:\n",
    "            base_image[300:350, 300:400] *= 2.0  # New development\n",
    "        \n",
    "        # Save as GeoTIFF\n",
    "        filename = f\"{download_dir}/simulated_sar_{i+1}.tif\"\n",
    "        \n",
    "        # Create rasterio profile\n",
    "        profile = {\n",
    "            'driver': 'GTiff',\n",
    "            'height': base_image.shape[0],\n",
    "            'width': base_image.shape[1],\n",
    "            'count': 1,\n",
    "            'dtype': base_image.dtype,\n",
    "            'crs': 'EPSG:4326',\n",
    "            'transform': rasterio.transform.from_bounds(\n",
    "                aoi_bbox[0], aoi_bbox[1], aoi_bbox[2], aoi_bbox[3],\n",
    "                base_image.shape[1], base_image.shape[0]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        with rasterio.open(filename, 'w', **profile) as dst:\n",
    "            dst.write(base_image, 1)\n",
    "        \n",
    "        downloaded_files.append(filename)\n",
    "    \n",
    "    print(f\"Created {len(downloaded_files)} simulated files for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess SAR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess downloaded SAR data\n",
    "print('Preprocessing SAR data...')\n",
    "\n",
    "processed_files = []\n",
    "\n",
    "for i, file_path in enumerate(downloaded_files):\n",
    "    try:\n",
    "        print(f\"Processing file {i+1}/{len(downloaded_files)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # For real Sentinel-1 data, you would apply:\n",
    "        # - Radiometric calibration\n",
    "        # - Terrain correction\n",
    "        # - Speckle filtering\n",
    "        \n",
    "        # For this demo, we'll apply basic preprocessing\n",
    "        with rasterio.open(file_path) as src:\n",
    "            image_data = src.read(1)\n",
    "            profile = src.profile\n",
    "        \n",
    "        # Apply basic preprocessing\n",
    "        # 1. Convert to dB scale\n",
    "        image_db = 10 * np.log10(image_data + 1e-10)\n",
    "        \n",
    "        # 2. Apply simple speckle filter (moving average)\n",
    "        from scipy import ndimage\n",
    "        filtered_image = ndimage.uniform_filter(image_db, size=3)\n",
    "        \n",
    "        # 3. Normalize to reasonable range\n",
    "        filtered_image = np.clip(filtered_image, -25, 5)\n",
    "        \n",
    "        # Save processed image\n",
    "        processed_filename = file_path.replace('.tif', '_processed.tif')\n",
    "        \n",
    "        profile.update(dtype=filtered_image.dtype)\n",
    "        with rasterio.open(processed_filename, 'w', **profile) as dst:\n",
    "            dst.write(filtered_image, 1)\n",
    "        \n",
    "        processed_files.append(processed_filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully processed {len(processed_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Visualize Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed images\n",
    "images = []\n",
    "dates = []\n",
    "\n",
    "for i, file_path in enumerate(processed_files):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        image_data = src.read(1)\n",
    "        images.append(image_data)\n",
    "        # For demo, create synthetic dates\n",
    "        date = datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=i*30)\n",
    "        dates.append(date.strftime('%Y-%m-%d'))\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")\n",
    "print(f\"Image shape: {images[0].shape}\")\n",
    "print(f\"Dates: {dates}\")\n",
    "\n",
    "# Visualize the time series\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (img, date) in enumerate(zip(images[:6], dates[:6])):\n",
    "    if i < len(axes):\n",
    "        im = axes[i].imshow(img, cmap='gray', vmin=-20, vmax=0)\n",
    "        axes[i].set_title(f\"SAR Image {i+1}\\n{date}\")\n",
    "        axes[i].set_axis_off()\n",
    "        plt.colorbar(im, ax=axes[i], label='dB')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(images), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('SAR Time Series', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Change Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images to linear scale for change detection\n",
    "linear_images = []\n",
    "for img in images:\n",
    "    # Convert from dB back to linear scale\n",
    "    linear_img = 10 ** (img / 10)\n",
    "    linear_images.append(linear_img)\n",
    "\n",
    "# Stack images into 3D array (time, height, width)\n",
    "image_stack = np.stack(linear_images, axis=0)\n",
    "print(f\"Image stack shape: {image_stack.shape}\")\n",
    "\n",
    "# Perform change detection using different methods\n",
    "print('\\nPerforming change detection...')\n",
    "\n",
    "# Method 1: Omnibus test\n",
    "print('1. Omnibus test...')\n",
    "omnibus_results = change_detection.detect_changes(\n",
    "    linear_images, \n",
    "    method='omnibus', \n",
    "    significance=0.05\n",
    ")\n",
    "\n",
    "# Method 2: Ratio test\n",
    "print('2. Ratio test...')\n",
    "ratio_results = change_detection.detect_changes(\n",
    "    linear_images, \n",
    "    method='ratio'\n",
    ")\n",
    "\n",
    "# Method 3: Difference test\n",
    "print('3. Difference test...')\n",
    "difference_results = change_detection.detect_changes(\n",
    "    linear_images, \n",
    "    method='difference'\n",
    ")\n",
    "\n",
    "print('Change detection completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Change Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize change detection results\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "methods = ['Omnibus Test', 'Ratio Test', 'Difference Test']\n",
    "results = [omnibus_results, ratio_results, difference_results]\n",
    "\n",
    "for i, (method, result) in enumerate(zip(methods, results)):\n",
    "    # First change map\n",
    "    im1 = axes[i, 0].imshow(result['first_change'], cmap='viridis')\n",
    "    axes[i, 0].set_title(f\"{method}\\nFirst Change\")\n",
    "    axes[i, 0].set_axis_off()\n",
    "    plt.colorbar(im1, ax=axes[i, 0], label='Time Step')\n",
    "    \n",
    "    # Change frequency map\n",
    "    im2 = axes[i, 1].imshow(result['change_frequency'], cmap='hot')\n",
    "    axes[i, 1].set_title(f\"{method}\\nChange Frequency\")\n",
    "    axes[i, 1].set_axis_off()\n",
    "    plt.colorbar(im2, ax=axes[i, 1], label='# Changes')\n",
    "    \n",
    "    # Change magnitude map\n",
    "    im3 = axes[i, 2].imshow(result['change_magnitude'], cmap='jet')\n",
    "    axes[i, 2].set_title(f\"{method}\\nChange Magnitude\")\n",
    "    axes[i, 2].set_axis_off()\n",
    "    plt.colorbar(im3, ax=axes[i, 2], label='Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Change Detection Results Comparison', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Analysis of Change Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on omnibus test results for detailed analysis\n",
    "change_data = omnibus_results\n",
    "\n",
    "# Calculate change statistics\n",
    "total_pixels = change_data['first_change'].size\n",
    "changed_pixels = np.sum(change_data['first_change'] > 0)\n",
    "change_percentage = (changed_pixels / total_pixels) * 100\n",
    "\n",
    "print(f\"Change Detection Statistics (Omnibus Test):\")\n",
    "print(f\"Total pixels: {total_pixels:,}\")\n",
    "print(f\"Changed pixels: {changed_pixels:,}\")\n",
    "print(f\"Change percentage: {change_percentage:.2f}%\")\n",
    "\n",
    "# Find areas with highest change frequency\n",
    "max_frequency = np.max(change_data['change_frequency'])\n",
    "high_change_areas = change_data['change_frequency'] >= max_frequency * 0.8\n",
    "\n",
    "print(f\"\\nAreas with high change frequency (>= {max_frequency * 0.8:.1f}):\")\n",
    "print(f\"Number of pixels: {np.sum(high_change_areas):,}\")\n",
    "\n",
    "# Create a combined change map\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original first image\n",
    "axes[0, 0].imshow(images[0], cmap='gray', vmin=-20, vmax=0)\n",
    "axes[0, 0].set_title('First SAR Image')\n",
    "axes[0, 0].set_axis_off()\n",
    "\n",
    "# Original last image\n",
    "axes[0, 1].imshow(images[-1], cmap='gray', vmin=-20, vmax=0)\n",
    "axes[0, 1].set_title('Last SAR Image')\n",
    "axes[0, 1].set_axis_off()\n",
    "\n",
    "# Change frequency overlay\n",
    "axes[1, 0].imshow(images[0], cmap='gray', vmin=-20, vmax=0, alpha=0.7)\n",
    "change_overlay = np.ma.masked_where(change_data['change_frequency'] == 0, \n",
    "                                   change_data['change_frequency'])\n",
    "axes[1, 0].imshow(change_overlay, cmap='Reds', alpha=0.8)\n",
    "axes[1, 0].set_title('Changes Overlaid on First Image')\n",
    "axes[1, 0].set_axis_off()\n",
    "\n",
    "# High change areas\n",
    "axes[1, 1].imshow(high_change_areas, cmap='binary')\n",
    "axes[1, 1].set_title('High Change Areas')\n",
    "axes[1, 1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Time Series Analysis at Specific Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select points for time series analysis\n",
    "# Find points with changes\n",
    "change_locations = np.where(change_data['change_frequency'] > 0)\n",
    "\n",
    "if len(change_locations[0]) > 0:\n",
    "    # Select a few representative points\n",
    "    n_points = min(5, len(change_locations[0]))\n",
    "    indices = np.random.choice(len(change_locations[0]), n_points, replace=False)\n",
    "    \n",
    "    selected_points = [(change_locations[0][i], change_locations[1][i]) for i in indices]\n",
    "else:\n",
    "    # If no changes detected, select random points\n",
    "    selected_points = [\n",
    "        (100, 100), (200, 200), (300, 300), (150, 250), (250, 150)\n",
    "    ]\n",
    "\n",
    "print(f\"Selected points for time series analysis: {selected_points}\")\n",
    "\n",
    "# Extract time series for selected points\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (row, col) in enumerate(selected_points[:6]):\n",
    "    if row < image_stack.shape[1] and col < image_stack.shape[2]:\n",
    "        # Extract time series (convert back to dB for plotting)\n",
    "        time_series = 10 * np.log10(image_stack[:, row, col] + 1e-10)\n",
    "        \n",
    "        axes[i].plot(range(len(time_series)), time_series, 'o-', linewidth=2, markersize=6)\n",
    "        axes[i].set_title(f\"Point ({row}, {col})\")\n",
    "        axes[i].set_xlabel('Time Step')\n",
    "        axes[i].set_ylabel('Backscatter (dB)')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight change points if any\n",
    "        if change_data['first_change'][row, col] > 0:\n",
    "            change_time = change_data['first_change'][row, col] - 1\n",
    "            axes[i].axvline(x=change_time, color='red', linestyle='--', \n",
    "                          label=f\"First change at t={change_time+1}\")\n",
    "            axes[i].legend()\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(selected_points), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('SAR Backscatter Time Series at Selected Points', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = 'change_detection_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save change detection results as GeoTIFF files\n",
    "print('Saving results...')\n",
    "\n",
    "# Get geospatial information from the first processed file\n",
    "with rasterio.open(processed_files[0]) as src:\n",
    "    profile = src.profile\n",
    "    profile.update(count=1)\n",
    "\n",
    "# Save omnibus test results\n",
    "for key, data in omnibus_results.items():\n",
    "    if key != 'metadata' and isinstance(data, np.ndarray):\n",
    "        output_file = os.path.join(results_dir, f\"omnibus_{key}.tif\")\n",
    "        \n",
    "        profile_copy = profile.copy()\n",
    "        profile_copy.update(dtype=data.dtype)\n",
    "        \n",
    "        with rasterio.open(output_file, 'w', **profile_copy) as dst:\n",
    "            dst.write(data, 1)\n",
    "        \n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_file = os.path.join(results_dir, 'change_detection_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"SAR Change Detection Results Summary\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    f.write(f\"Area of Interest: {aoi_bbox}\\n\")\n",
    "    f.write(f\"Time Period: {start_date} to {end_date}\\n\")\n",
    "    f.write(f\"Number of Images: {len(images)}\\n\")\n",
    "    f.write(f\"Image Dimensions: {images[0].shape}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Change Detection Statistics (Omnibus Test):\\n\")\n",
    "    f.write(f\"Total pixels: {total_pixels:,}\\n\")\n",
    "    f.write(f\"Changed pixels: {changed_pixels:,}\\n\")\n",
    "    f.write(f\"Change percentage: {change_percentage:.2f}%\\n\")\n",
    "    f.write(f\"Maximum change frequency: {max_frequency}\\n\")\n",
    "\n",
    "print(f\"Summary saved: {summary_file}\")\n",
    "\n",
    "# Create final visualization\n",
    "fig = visualization.plot_changes(omnibus_results, \n",
    "                                output_file=os.path.join(results_dir, 'change_detection_overview.png'))\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\nAll results saved to: {results_dir}/\")\n",
    "print(\"\\nChange detection analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete workflow for urban change detection using real Sentinel-1 SAR data:\n",
    "\n",
    "1. **Data Download**: Searched and downloaded Sentinel-1 GRD products from Copernicus Hub\n",
    "2. **Preprocessing**: Applied radiometric calibration, terrain correction, and speckle filtering\n",
    "3. **Change Detection**: Used three different algorithms (Omnibus, Ratio, Difference tests)\n",
    "4. **Analysis**: Calculated change statistics and identified high-change areas\n",
    "5. **Visualization**: Created comprehensive plots and time series analysis\n",
    "6. **Export**: Saved results as GeoTIFF files and summary statistics\n",
    "\n",
    "### Key Findings:\n",
    "- The omnibus test provides the most statistically robust change detection\n",
    "- Change frequency maps help identify areas of persistent change\n",
    "- Time series analysis reveals the temporal patterns of changes\n",
    "\n",
    "### Next Steps:\n",
    "- Apply to larger areas or longer time periods\n",
    "- Integrate with optical data for improved interpretation\n",
    "- Implement automated change classification\n",
    "- Add validation with ground truth data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}