{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitemporal Urban Change Detection in New Delhi using Sentinel-1 SAR Data\n",
    "\n",
    "This notebook demonstrates multitemporal change detection in New Delhi using Sentinel-1 SAR data accessed through the openEO API. Unlike bi-temporal analysis, this approach analyzes multiple time periods to create:\n",
    "\n",
    "1. **Change Frequency Map**: Shows how often changes occur at each pixel location\n",
    "2. **Time-series Analysis**: Displays backscatter evolution for specific pixels over time\n",
    "\n",
    "## Requirements\n",
    "- SAR4CET toolkit (will be installed below)\n",
    "- openEO Python client\n",
    "- Copernicus Dataspace account for authentication\n",
    "- xarray, matplotlib, numpy for data processing and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Installation\n",
    "\n",
    "First, let's clone the SAR4CET repository and install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the SAR4CET repository\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if SAR4CET directory already exists\n",
    "if not os.path.exists('SAR4CET'):\n",
    "    print('Cloning SAR4CET repository...')\n",
    "    result = subprocess.run(['git', 'clone', 'https://github.com/naikp13/SAR4CET.git'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print('Repository cloned successfully!')\n",
    "    else:\n",
    "        print(f'Error cloning repository: {result.stderr}')\n",
    "else:\n",
    "    print('SAR4CET directory already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements from requirements.txt\n",
    "print('Installing SAR4CET requirements...')\n",
    "\n",
    "# Change to SAR4CET directory and install requirements\n",
    "if os.path.exists('SAR4CET/requirements.txt'):\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'SAR4CET/requirements.txt'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print('Requirements installed successfully!')\n",
    "        print('Installed packages from requirements.txt')\n",
    "    else:\n",
    "        print(f'Error installing requirements: {result.stderr}')\n",
    "        print('Trying to install individual packages...')\n",
    "        # Try installing key packages individually\n",
    "        key_packages = ['openeo>=0.22.0', 'xarray>=0.19.0', 'numpy>=1.20.0', \n",
    "                       'matplotlib>=3.4.0', 'rasterio>=1.2.0', 'scipy>=1.7.0']\n",
    "        for package in key_packages:\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', package])\n",
    "else:\n",
    "    print('requirements.txt not found, installing key packages manually...')\n",
    "    key_packages = ['openeo>=0.22.0', 'xarray>=0.19.0', 'numpy>=1.20.0', \n",
    "                   'matplotlib>=3.4.0', 'rasterio>=1.2.0', 'scipy>=1.7.0']\n",
    "    for package in key_packages:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SAR4CET to Python path for imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add SAR4CET directory to Python path\n",
    "sar4cet_path = os.path.abspath('SAR4CET')\n",
    "if sar4cet_path not in sys.path:\n",
    "    sys.path.insert(0, sar4cet_path)\n",
    "    print(f'Added {sar4cet_path} to Python path')\n",
    "\n",
    "# Verify installation by importing key modules\n",
    "try:\n",
    "    import openeo\n",
    "    import xarray as xr\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from scipy import ndimage\n",
    "    print('All required packages imported successfully!')\n",
    "    print(f'openEO version: {openeo.__version__}')\n",
    "    print(f'xarray version: {xr.__version__}')\n",
    "except ImportError as e:\n",
    "    print(f'Import error: {e}')\n",
    "    print('Please install missing packages manually.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll need for the multitemporal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib parameters for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Optional: Import SAR4CET modules if available\n",
    "try:\n",
    "    from sar4cet import preprocessing, visualization\n",
    "    print('SAR4CET modules imported successfully!')\n",
    "except ImportError:\n",
    "    print('SAR4CET modules not available, using standalone approach.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to openEO Backend\n",
    "\n",
    "Connect to the Copernicus Dataspace openEO backend and authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Copernicus Dataspace openEO backend\n",
    "backend = 'openeo.dataspace.copernicus.eu'\n",
    "conn = openeo.connect(backend).authenticate_oidc()\n",
    "print(f'Connected to {backend}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Area of Interest and Time Periods\n",
    "\n",
    "We'll focus on the same developing area in South Delhi but analyze multiple time periods over several years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area of Interest: Developing area in South Delhi (Gurgaon-Delhi border)\n",
    "spatial_extent = {\n",
    "    'west': 77.0500,\n",
    "    'east': 77.1200,\n",
    "    'south': 28.4200,\n",
    "    'north': 28.4800,\n",
    "    'crs': 'EPSG:4326',\n",
    "}\n",
    "\n",
    "# Define multiple time periods for multitemporal analysis\n",
    "# We'll analyze quarterly periods over 3 years (2021-2023)\n",
    "time_periods = [\n",
    "    ['2021-01-01', '2021-03-31'],  # Q1 2021\n",
    "    ['2021-04-01', '2021-06-30'],  # Q2 2021\n",
    "    ['2021-07-01', '2021-09-30'],  # Q3 2021\n",
    "    ['2021-10-01', '2021-12-31'], # Q4 2021\n",
    "    ['2022-01-01', '2022-03-31'],  # Q1 2022\n",
    "    ['2022-04-01', '2022-06-30'],  # Q2 2022\n",
    "    ['2022-07-01', '2022-09-30'],  # Q3 2022\n",
    "    ['2022-10-01', '2022-12-31'], # Q4 2022\n",
    "    ['2023-01-01', '2023-03-31'],  # Q1 2023\n",
    "    ['2023-04-01', '2023-06-30'],  # Q2 2023\n",
    "    ['2023-07-01', '2023-09-30'],  # Q3 2023\n",
    "    ['2023-10-01', '2023-12-31'], # Q4 2023\n",
    "]\n",
    "\n",
    "print(f'Area of Interest: {spatial_extent}')\n",
    "print(f'Number of time periods: {len(time_periods)}')\n",
    "print(f'Time range: {time_periods[0][0]} to {time_periods[-1][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Multitemporal Sentinel-1 Data\n",
    "\n",
    "Load Sentinel-1 data for all time periods and apply SAR backscatter processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process SAR data for a given time period\n",
    "def load_and_process_sar(conn, spatial_extent, temporal_extent, period_name):\n",
    "    '''Load and process SAR data for a specific time period'''\n",
    "    print(f'Processing {period_name}: {temporal_extent[0]} to {temporal_extent[1]}')\n",
    "    \n",
    "    # Load Sentinel-1 data\n",
    "    s1 = conn.load_collection(\n",
    "        'SENTINEL1_GRD',\n",
    "        spatial_extent=spatial_extent,\n",
    "        bands=['VV', 'VH'],\n",
    "        temporal_extent=temporal_extent,\n",
    "        properties={'sat:orbit_state': lambda od: od == 'ASCENDING'},\n",
    "    )\n",
    "    \n",
    "    # Apply SAR backscatter processing\n",
    "    s1_scatter = s1.sar_backscatter(\n",
    "        coefficient='sigma0-ellipsoid', \n",
    "        elevation_model='COPERNICUS_30'\n",
    "    )\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    s1bs = s1_scatter.apply(lambda x: 10 * x.log(base=10))\n",
    "    \n",
    "    # Create median composite\n",
    "    s1_median = s1bs.median_time()\n",
    "    \n",
    "    return s1_median\n",
    "\n",
    "print('Function defined for SAR data processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all time periods (this may take some time)\n",
    "print('Starting multitemporal data processing...')\n",
    "print('Note: This process may take 10-15 minutes depending on data availability')\n",
    "\n",
    "processed_periods = []\n",
    "period_names = []\n",
    "\n",
    "for i, period in enumerate(time_periods):\n",
    "    period_name = f'Period_{i+1:02d}_{period[0][:7]}'  # e.g., Period_01_2021-01\n",
    "    period_names.append(period_name)\n",
    "    \n",
    "    try:\n",
    "        processed_data = load_and_process_sar(conn, spatial_extent, period, period_name)\n",
    "        processed_periods.append(processed_data)\n",
    "        print(f'✓ Successfully processed {period_name}')\n",
    "    except Exception as e:\n",
    "        print(f'✗ Error processing {period_name}: {str(e)}')\n",
    "        processed_periods.append(None)\n",
    "\n",
    "# Filter out None values\n",
    "valid_periods = [(name, data) for name, data in zip(period_names, processed_periods) if data is not None]\n",
    "print(f'\\nSuccessfully processed {len(valid_periods)} out of {len(time_periods)} periods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Processed Data\n",
    "\n",
    "Download the processed SAR data for all valid time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all processed periods\n",
    "downloaded_files = []\n",
    "\n",
    "print('Downloading processed data...')\n",
    "for period_name, processed_data in valid_periods:\n",
    "    filename = f'new_delhi_{period_name.lower()}.nc'\n",
    "    try:\n",
    "        print(f'Downloading {filename}...')\n",
    "        processed_data.download(filename)\n",
    "        downloaded_files.append((period_name, filename))\n",
    "        print(f'✓ Downloaded {filename}')\n",
    "    except Exception as e:\n",
    "        print(f'✗ Error downloading {filename}: {str(e)}')\n",
    "\n",
    "print(f'\\nDownloaded {len(downloaded_files)} files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Downloaded Data and Create Time Series\n",
    "\n",
    "Load all downloaded NetCDF files and organize them into a time series dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all downloaded files into a list\n",
    "datasets = []\n",
    "time_labels = []\n",
    "\n",
    "for period_name, filename in downloaded_files:\n",
    "    try:\n",
    "        ds = xr.open_dataset(filename)\n",
    "        datasets.append(ds)\n",
    "        time_labels.append(period_name)\n",
    "        print(f'Loaded {filename} - Shape: {ds.dims}')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {filename}: {str(e)}')\n",
    "\n",
    "print(f'\\nLoaded {len(datasets)} datasets for analysis')\n",
    "\n",
    "if len(datasets) > 0:\n",
    "    print(f'Dataset dimensions: {datasets[0].dims}')\n",
    "    print(f'Available bands: {list(datasets[0].data_vars)}')\n",
    "else:\n",
    "    print('No datasets available for analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multitemporal arrays for VV and VH\n",
    "if len(datasets) > 0:\n",
    "    # Extract VV and VH data from all time periods\n",
    "    vv_timeseries = np.stack([ds.VV.values for ds in datasets], axis=0)\n",
    "    vh_timeseries = np.stack([ds.VH.values for ds in datasets], axis=0)\n",
    "    \n",
    "    print(f'VV time series shape: {vv_timeseries.shape}')\n",
    "    print(f'VH time series shape: {vh_timeseries.shape}')\n",
    "    print(f'Time dimension: {len(time_labels)} periods')\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    x_coords = datasets[0].x.values\n",
    "    y_coords = datasets[0].y.values\n",
    "    \n",
    "    print(f'Spatial extent - X: {x_coords.min():.4f} to {x_coords.max():.4f}')\n",
    "    print(f'Spatial extent - Y: {y_coords.min():.4f} to {y_coords.max():.4f}')\n",
    "else:\n",
    "    print('Cannot create time series - no valid datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change Detection Analysis\n",
    "\n",
    "Perform multitemporal change detection to identify when and how often changes occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect changes between consecutive time periods\n",
    "def detect_temporal_changes(timeseries, threshold=2.0):\n",
    "    '''\n",
    "    Detect changes between consecutive time periods\n",
    "    \n",
    "    Parameters:\n",
    "    - timeseries: 3D array (time, y, x)\n",
    "    - threshold: Change threshold in dB\n",
    "    \n",
    "    Returns:\n",
    "    - change_mask: 3D boolean array indicating changes\n",
    "    - change_magnitude: 3D array with change magnitudes\n",
    "    '''\n",
    "    n_times, height, width = timeseries.shape\n",
    "    change_mask = np.zeros((n_times-1, height, width), dtype=bool)\n",
    "    change_magnitude = np.zeros((n_times-1, height, width))\n",
    "    \n",
    "    for t in range(n_times-1):\n",
    "        # Calculate difference between consecutive periods\n",
    "        diff = timeseries[t+1] - timeseries[t]\n",
    "        \n",
    "        # Apply threshold to detect significant changes\n",
    "        change_mask[t] = np.abs(diff) > threshold\n",
    "        change_magnitude[t] = diff\n",
    "    \n",
    "    return change_mask, change_magnitude\n",
    "\n",
    "if len(datasets) > 1:\n",
    "    # Detect changes for VV polarization\n",
    "    vv_change_mask, vv_change_magnitude = detect_temporal_changes(vv_timeseries, threshold=2.0)\n",
    "    \n",
    "    # Detect changes for VH polarization\n",
    "    vh_change_mask, vh_change_magnitude = detect_temporal_changes(vh_timeseries, threshold=1.5)\n",
    "    \n",
    "    print(f'VV change detection completed - Shape: {vv_change_mask.shape}')\n",
    "    print(f'VH change detection completed - Shape: {vh_change_mask.shape}')\n",
    "else:\n",
    "    print('Need at least 2 time periods for change detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Change Frequency Map\n",
    "\n",
    "Generate a map showing how frequently changes occur at each pixel location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(datasets) > 1:\n",
    "    # Calculate change frequency (number of times each pixel changed)\n",
    "    vv_change_frequency = np.sum(vv_change_mask, axis=0)\n",
    "    vh_change_frequency = np.sum(vh_change_mask, axis=0)\n",
    "    \n",
    "    # Combined change frequency (either VV or VH changed)\n",
    "    combined_change_mask = vv_change_mask | vh_change_mask\n",
    "    combined_change_frequency = np.sum(combined_change_mask, axis=0)\n",
    "    \n",
    "    # Calculate change frequency as percentage\n",
    "    max_possible_changes = len(datasets) - 1\n",
    "    vv_change_frequency_pct = (vv_change_frequency / max_possible_changes) * 100\n",
    "    vh_change_frequency_pct = (vh_change_frequency / max_possible_changes) * 100\n",
    "    combined_change_frequency_pct = (combined_change_frequency / max_possible_changes) * 100\n",
    "    \n",
    "    print(f'Maximum possible changes per pixel: {max_possible_changes}')\n",
    "    print(f'VV - Max changes observed: {vv_change_frequency.max()}')\n",
    "    print(f'VH - Max changes observed: {vh_change_frequency.max()}')\n",
    "    print(f'Combined - Max changes observed: {combined_change_frequency.max()}')\n",
    "    \n",
    "    # Statistics\n",
    "    total_pixels = vv_change_frequency.size\n",
    "    changed_pixels = np.sum(combined_change_frequency > 0)\n",
    "    print(f'\\nPixels that changed at least once: {changed_pixels} ({changed_pixels/total_pixels*100:.2f}%)')\n",
    "else:\n",
    "    print('Cannot create change frequency map - insufficient data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize change frequency maps\n",
    "if len(datasets) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # VV change frequency\n",
    "    im1 = axes[0, 0].imshow(vv_change_frequency_pct, cmap='Reds', vmin=0, vmax=100)\n",
    "    axes[0, 0].set_title('VV Change Frequency Map')\n",
    "    axes[0, 0].axis('off')\n",
    "    cbar1 = plt.colorbar(im1, ax=axes[0, 0], label='Change Frequency (%)')\n",
    "    \n",
    "    # VH change frequency\n",
    "    im2 = axes[0, 1].imshow(vh_change_frequency_pct, cmap='Blues', vmin=0, vmax=100)\n",
    "    axes[0, 1].set_title('VH Change Frequency Map')\n",
    "    axes[0, 1].axis('off')\n",
    "    cbar2 = plt.colorbar(im2, ax=axes[0, 1], label='Change Frequency (%)')\n",
    "    \n",
    "    # Combined change frequency\n",
    "    im3 = axes[1, 0].imshow(combined_change_frequency_pct, cmap='viridis', vmin=0, vmax=100)\n",
    "    axes[1, 0].set_title('Combined Change Frequency Map')\n",
    "    axes[1, 0].axis('off')\n",
    "    cbar3 = plt.colorbar(im3, ax=axes[1, 0], label='Change Frequency (%)')\n",
    "    \n",
    "    # Change frequency histogram\n",
    "    axes[1, 1].hist(combined_change_frequency_pct.flatten(), bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Change Frequency (%)')\n",
    "    axes[1, 1].set_ylabel('Number of Pixels')\n",
    "    axes[1, 1].set_title('Distribution of Change Frequencies')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nInterpretation:')\n",
    "    print('- Red/Blue areas: High change frequency (frequent urban development)')\n",
    "    print('- Dark areas: Low/no change frequency (stable areas)')\n",
    "    print('- Combined map shows overall change activity')\n",
    "else:\n",
    "    print('Cannot visualize change frequency maps - insufficient data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time-Series Analysis for Specific Pixels\n",
    "\n",
    "Select specific pixels and analyze their backscatter evolution over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select interesting pixels for time-series analysis\n",
    "def select_analysis_pixels(change_frequency_map, n_pixels=5):\n",
    "    '''\n",
    "    Select pixels with different change characteristics for analysis\n",
    "    \n",
    "    Returns:\n",
    "    - List of (row, col, description) tuples\n",
    "    '''\n",
    "    height, width = change_frequency_map.shape\n",
    "    pixels = []\n",
    "    \n",
    "    # 1. Pixel with highest change frequency\n",
    "    max_change_idx = np.unravel_index(np.argmax(change_frequency_map), change_frequency_map.shape)\n",
    "    pixels.append((max_change_idx[0], max_change_idx[1], 'Highest Change Frequency'))\n",
    "    \n",
    "    # 2. Pixel with no changes (stable)\n",
    "    stable_pixels = np.where(change_frequency_map == 0)\n",
    "    if len(stable_pixels[0]) > 0:\n",
    "        stable_idx = len(stable_pixels[0]) // 2  # Pick middle stable pixel\n",
    "        pixels.append((stable_pixels[0][stable_idx], stable_pixels[1][stable_idx], 'Stable (No Changes)'))\n",
    "    \n",
    "    # 3. Pixel with moderate change frequency\n",
    "    moderate_threshold = np.percentile(change_frequency_map[change_frequency_map > 0], 50)\n",
    "    moderate_pixels = np.where(np.abs(change_frequency_map - moderate_threshold) < 5)\n",
    "    if len(moderate_pixels[0]) > 0:\n",
    "        mod_idx = 0\n",
    "        pixels.append((moderate_pixels[0][mod_idx], moderate_pixels[1][mod_idx], 'Moderate Changes'))\n",
    "    \n",
    "    # 4. Random pixels from different quadrants\n",
    "    quadrants = [\n",
    "        (height//4, width//4, 'NW Quadrant'),\n",
    "        (height//4, 3*width//4, 'NE Quadrant'),\n",
    "        (3*height//4, width//4, 'SW Quadrant'),\n",
    "        (3*height//4, 3*width//4, 'SE Quadrant')\n",
    "    ]\n",
    "    \n",
    "    for row, col, desc in quadrants[:2]:  # Add 2 quadrant pixels\n",
    "        if len(pixels) < n_pixels:\n",
    "            pixels.append((row, col, desc))\n",
    "    \n",
    "    return pixels[:n_pixels]\n",
    "\n",
    "if len(datasets) > 1:\n",
    "    # Select pixels for analysis\n",
    "    analysis_pixels = select_analysis_pixels(combined_change_frequency_pct, n_pixels=5)\n",
    "    \n",
    "    print('Selected pixels for time-series analysis:')\n",
    "    for i, (row, col, desc) in enumerate(analysis_pixels):\n",
    "        change_freq = combined_change_frequency_pct[row, col]\n",
    "        print(f'{i+1}. {desc}: Row {row}, Col {col} (Change freq: {change_freq:.1f}%)')\n",
    "else:\n",
    "    print('Cannot select pixels for analysis - insufficient data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-series plots for selected pixels\n",
    "if len(datasets) > 1:\n",
    "    # Create time axis\n",
    "    time_axis = range(len(time_labels))\n",
    "    \n",
    "    # Create subplots for time-series\n",
    "    fig, axes = plt.subplots(len(analysis_pixels), 2, figsize=(16, 4*len(analysis_pixels)))\n",
    "    \n",
    "    if len(analysis_pixels) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (row, col, desc) in enumerate(analysis_pixels):\n",
    "        # Extract time series for this pixel\n",
    "        vv_ts = vv_timeseries[:, row, col]\n",
    "        vh_ts = vh_timeseries[:, row, col]\n",
    "        \n",
    "        # Plot VV time series\n",
    "        axes[i, 0].plot(time_axis, vv_ts, 'o-', color='red', linewidth=2, markersize=6)\n",
    "        axes[i, 0].set_title(f'{desc} - VV Polarization\\nPixel ({row}, {col})')\n",
    "        axes[i, 0].set_ylabel('Backscatter (dB)')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        axes[i, 0].set_xticks(time_axis[::2])  # Show every other label\n",
    "        axes[i, 0].set_xticklabels([time_labels[j] for j in time_axis[::2]], rotation=45)\n",
    "        \n",
    "        # Plot VH time series\n",
    "        axes[i, 1].plot(time_axis, vh_ts, 'o-', color='blue', linewidth=2, markersize=6)\n",
    "        axes[i, 1].set_title(f'{desc} - VH Polarization\\nPixel ({row}, {col})')\n",
    "        axes[i, 1].set_ylabel('Backscatter (dB)')\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        axes[i, 1].set_xticks(time_axis[::2])\n",
    "        axes[i, 1].set_xticklabels([time_labels[j] for j in time_axis[::2]], rotation=45)\n",
    "        \n",
    "        # Add statistics\n",
    "        vv_mean, vv_std = np.mean(vv_ts), np.std(vv_ts)\n",
    "        vh_mean, vh_std = np.mean(vh_ts), np.std(vh_ts)\n",
    "        \n",
    "        axes[i, 0].text(0.02, 0.98, f'Mean: {vv_mean:.2f}±{vv_std:.2f} dB', \n",
    "                       transform=axes[i, 0].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        axes[i, 1].text(0.02, 0.98, f'Mean: {vh_mean:.2f}±{vh_std:.2f} dB', \n",
    "                       transform=axes[i, 1].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nTime-series Analysis Interpretation:')\n",
    "    print('- Sudden increases in backscatter may indicate new construction')\n",
    "    print('- Gradual changes may indicate ongoing development')\n",
    "    print('- Stable values indicate no significant land cover changes')\n",
    "    print('- VV is more sensitive to building structures')\n",
    "    print('- VH is more sensitive to vegetation and surface roughness changes')\n",
    "else:\n",
    "    print('Cannot create time-series plots - insufficient data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "This multitemporal analysis provides comprehensive insights into urban development patterns in New Delhi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "if len(datasets) > 1:\n",
    "    print('=== MULTITEMPORAL CHANGE DETECTION SUMMARY ===')\n",
    "    print(f'Analysis period: {time_labels[0]} to {time_labels[-1]}')\n",
    "    print(f'Number of time periods analyzed: {len(datasets)}')\n",
    "    print(f'Spatial extent: {spatial_extent}')\n",
    "    print(f'Image dimensions: {vv_timeseries.shape[1]} x {vv_timeseries.shape[2]} pixels')\n",
    "    \n",
    "    # Change frequency statistics\n",
    "    total_pixels = combined_change_frequency.size\n",
    "    changed_pixels = np.sum(combined_change_frequency > 0)\n",
    "    high_change_pixels = np.sum(combined_change_frequency_pct > 50)\n",
    "    \n",
    "    print('\\n=== CHANGE FREQUENCY ANALYSIS ===')\n",
    "    print(f'Total pixels: {total_pixels:,}')\n",
    "    print(f'Pixels with changes: {changed_pixels:,} ({changed_pixels/total_pixels*100:.1f}%)')\n",
    "    print(f'Pixels with high change frequency (>50%): {high_change_pixels:,} ({high_change_pixels/total_pixels*100:.1f}%)')\n",
    "    print(f'Average change frequency: {np.mean(combined_change_frequency_pct):.1f}%')\n",
    "    \n",
    "    print('\\n=== KEY FINDINGS ===')\n",
    "    print('1. Urban development is most active in areas with high change frequency')\n",
    "    print('2. Time-series analysis reveals both gradual and sudden changes')\n",
    "    print('3. VV polarization is more sensitive to building construction')\n",
    "    print('4. VH polarization captures vegetation and surface texture changes')\n",
    "    print('5. Multitemporal approach provides more robust change detection than bi-temporal')\n",
    "    \n",
    "    print('\\n=== APPLICATIONS ===')\n",
    "    print('- Urban planning and monitoring')\n",
    "    print('- Infrastructure development tracking')\n",
    "    print('- Environmental impact assessment')\n",
    "    print('- Policy making for sustainable development')\n",
    "else:\n",
    "    print('Insufficient data for comprehensive summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up downloaded files (optional)\n",
    "import os\n",
    "\n",
    "print('\\n=== CLEANUP ===')\n",
    "print('Downloaded files:')\n",
    "for period_name, filename in downloaded_files:\n",
    "    if os.path.exists(filename):\n",
    "        file_size = os.path.getsize(filename) / (1024*1024)  # MB\n",
    "        print(f'  {filename} ({file_size:.1f} MB)')\n",
    "\n",
    "# Uncomment the following lines to remove downloaded files\n",
    "# for period_name, filename in downloaded_files:\n",
    "#     if os.path.exists(filename):\n",
    "#         os.remove(filename)\n",
    "# print('Downloaded files cleaned up')\n",
    "\n",
    "print('\\nMultitemporal analysis completed successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}